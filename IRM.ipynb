{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.special import beta as Beta\n",
    "import numpy.random as npr\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from sortedcontainers import SortedSet\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sampler(Enum):\n",
    "    GIBBS = 0\n",
    "    CLIMB = 1\n",
    "    ANNEAL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class model(object):\n",
    "    \"\"\"Infinite Relational Model.\"\"\"\n",
    "    def __init__(self, gamma, alpha, beta, relations):\n",
    "        # CRP gamma param for z assignments\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # beta param for η\n",
    "        self.beta = beta\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # list of relations \n",
    "        # a relation is a tuple of type (object,object,bool)\n",
    "        self.relations = relations\n",
    "        \n",
    "        # list of unique objects\n",
    "        objs = set()\n",
    "        for (a,b,_) in self.relations:\n",
    "            objs.add(a)\n",
    "            objs.add(b)\n",
    "        self.objs  = list(objs)\n",
    "        \n",
    "        # total number of objects\n",
    "        self.num_objs = len(self.objs)\n",
    "        \n",
    "        # denominator of CRP prob for joint table assignment (γ)(γ+1)...(γ+n-1)\n",
    "        # self.CRProb_joint_denom = np.prod(np.arange(self.gamma, self.gamma + self.num_objs, 1))\n",
    "        self.CRProb_joint_denom = np.prod(np.arange(self.gamma, self.gamma + self.num_objs))\n",
    "        \n",
    "        # denominator of CRP prob for last customer's table assignment (γ+n-1)\n",
    "        self.CRProb_cond_denom = self.gamma + self.num_objs - 1\n",
    "        \n",
    "        self.cluster_ids = SortedSet(range(self.num_objs))\n",
    "        # cluster assignements for objects, initialized to 0 for all\n",
    "        # self.z = np.zeros(self.num_objs, dtype=\"int\")\n",
    "        self.z = {}\n",
    "        \n",
    "        cluster_id = self.cluster_ids.pop(index = 0)\n",
    "\n",
    "        # set of all object types \n",
    "        self.obj_types = set([cluster_id])\n",
    "\n",
    "        self.obj_type_mem = defaultdict(set)\n",
    "        \n",
    "        for obj in self.objs:\n",
    "            self.z[obj] = cluster_id\n",
    "            self.obj_type_mem[cluster_id].add(obj)\n",
    "        \n",
    "        # number of objects in each object type\n",
    "        self.obj_type_count = Counter()\n",
    "        self.obj_type_count[cluster_id] += self.num_objs\n",
    "        \n",
    "        # graph of relations described by self.relations\n",
    "        # each object has four lists: 0 - outgoing with 0, 1 - outgoing with 1,\n",
    "        # 2 - incoming with 0, 3 - incoming in 1, 4 - does the object interact with itself?\n",
    "        self.graph = {}\n",
    "        \n",
    "        for obj in self.objs:\n",
    "            self.graph[obj] = [[] for _ in range(4)]\n",
    "            self.graph[obj].append(None)\n",
    "    \n",
    "        # counts of 0 and 1 edges between different pairs of object types\n",
    "        # 0 - m_bar, 1 - m\n",
    "        self.m = Counter()\n",
    "        self.m_bar = Counter()\n",
    "        \n",
    "        # keep track of (object, object) pairs\n",
    "        # allows for faster calculation of P(R|z)\n",
    "        self.pair_count = Counter()\n",
    "        self.pairs = set([(0,0)])\n",
    "        \n",
    "        # build graph and update counters\n",
    "        for (i, j, b) in self.relations:\n",
    "            if i == j:\n",
    "                self.graph[i][4] = b\n",
    "            elif b:\n",
    "                self.graph[i][1].append(j)\n",
    "                self.graph[j][3].append(i)\n",
    "            else:\n",
    "                self.graph[i][0].append(j)\n",
    "                self.graph[j][2].append(i)\n",
    "                \n",
    "            if b:\n",
    "                self.m[(0,0)] += 1\n",
    "            else:\n",
    "                self.m_bar[(0,0)] += 1\n",
    "            \n",
    "            self.pair_count[(0,0)] += 1\n",
    "        \n",
    "        # maximum joint prob as measure of max posterior \n",
    "        # p(z|R) ∝ p(z, R) = p(R|z)p(z)\n",
    "        self.max_prob = -np.inf\n",
    "        self.best_z = deepcopy(self.z)\n",
    "    \n",
    "    def backup(self):\n",
    "        \"\"\"Create a backup of current state.\"\"\"\n",
    "        self.backup_cluster_ids = deepcopy(self.cluster_ids)\n",
    "        self.backup_z = deepcopy(self.z)\n",
    "        self.backup_obj_types = deepcopy(self.obj_types)\n",
    "        self.backup_obj_type_count = deepcopy(self.obj_type_count)\n",
    "        self.backup_obj_type_mem = deepcopy(self.obj_type_mem)\n",
    "        self.backup_m = deepcopy(self.m)\n",
    "        self.backup_m_bar = deepcopy(self.m_bar)\n",
    "        self.backup_pair_count = deepcopy(self.pair_count)\n",
    "        self.backup_pairs = deepcopy(self.pairs)\n",
    "        \n",
    "    def restore(self):\n",
    "        \"\"\"Restore state to backup configurations.\"\"\"\n",
    "        self.cluster_ids = deepcopy(self.backup_cluster_ids)\n",
    "        self.z = deepcopy(self.backup_z)\n",
    "        self.obj_types = deepcopy(self.backup_obj_types)\n",
    "        self.obj_type_count = deepcopy(self.backup_obj_type_count)\n",
    "        self.obj_type_mem = deepcopy(self.backup_obj_type_mem)\n",
    "        self.m = deepcopy(self.backup_m)\n",
    "        self.m_bar = deepcopy(self.backup_m_bar)\n",
    "        self.pair_count = deepcopy(self.backup_pair_count)\n",
    "        self.pairs = deepcopy(self.backup_pairs)\n",
    "        \n",
    "    def empty_backup(self):\n",
    "        \"\"\"Empty backup.\"\"\"\n",
    "        del self.backup_cluster_ids, self.backup_z, self.backup_obj_types\n",
    "        del self.backup_obj_type_count, self.backup_m, self.backup_m_bar\n",
    "        del self.backup_pair_count, self.backup_pairs, self.backup_obj_type_mem\n",
    "        \n",
    "    def obj_type_inc(self, obj_type):\n",
    "        \"\"\"Increment the number of objects with given type.\n",
    "        \n",
    "        Add to set of object types if new.\n",
    "        \"\"\"\n",
    "        self.obj_type_count[obj_type] += 1\n",
    "        if self.obj_type_count[obj_type] == 1:\n",
    "            self.obj_types.add(obj_type)\n",
    "            self.cluster_ids.remove(obj_type)\n",
    "    \n",
    "    def obj_type_dec(self, obj_type):\n",
    "        \"\"\"Decrement the number of objects with given type.\n",
    "        \n",
    "        Remove from set of object types if no objects belong to type.\n",
    "        \"\"\"\n",
    "        self.obj_type_count[obj_type] -= 1\n",
    "        if self.obj_type_count[obj_type] == 0:\n",
    "            self.cluster_ids.add(obj_type)\n",
    "            self.obj_types.remove(obj_type)\n",
    "            del self.obj_type_count[obj_type]\n",
    "            del self.obj_type_mem[obj_type]\n",
    "            \n",
    "    def pair_count_update(self, pair, count):\n",
    "        \"\"\"Update the number of object type pairs with edges.\n",
    "        \n",
    "        This allows us to more efficiently compute p(R|z) which needs \n",
    "        iteration through all object type pairs. By only iterating \n",
    "        through pairs with non-zero m or m_bar values, we potentially\n",
    "        save time.\n",
    "        \"\"\"\n",
    "        self.pair_count[pair] += count\n",
    "        \n",
    "        if self.pair_count[pair] > 0 and self.pair_count[pair] == count:\n",
    "            self.pairs.add(pair)\n",
    "        elif self.pair_count[pair] == 0:\n",
    "            self.pairs.remove(pair)\n",
    "            del self.pair_count[pair]\n",
    "    \n",
    "    def update_ms(self, m, pair, count):\n",
    "        \"\"\"Update the number of object type pairs with edges of type 1\n",
    "        (m) and type 0(m_bar).\n",
    "        \"\"\"\n",
    "        if m:\n",
    "            self.m[pair] += count\n",
    "            if self.m[pair] == 0:\n",
    "                del self.m[pair]\n",
    "        else:\n",
    "            self.m_bar[pair] += count\n",
    "            if self.m_bar[pair] == 0:\n",
    "                del self.m_bar[pair]\n",
    "    \n",
    "        self.pair_count_update(pair, count)\n",
    "            \n",
    "    def deassign(self, obj):\n",
    "        \"\"\"Deassign given object.\n",
    "        \n",
    "        This is done in preparation for sampling the cluster assignment\n",
    "        of the given object. All graph counters are updated correspondingly.\n",
    "        \"\"\"\n",
    "        curr = obj\n",
    "        curr_type = self.z[obj]\n",
    "\n",
    "        m = Counter()\n",
    "        m_bar = Counter()\n",
    "        \n",
    "        self.z[curr] = None\n",
    "        self.obj_type_mem[curr_type].remove(obj)\n",
    "        self.obj_type_dec(curr_type)\n",
    "\n",
    "        if self.graph[curr][4] == 1:    \n",
    "            self.update_ms(1, (curr_type, curr_type), -1)\n",
    "            m[(curr_type, curr_type)] -= 1\n",
    "        elif self.graph[curr][4] == 0:\n",
    "            self.update_ms(0, (curr_type, curr_type), -1)\n",
    "            m_bar[(curr_type, curr_type)] -= 1\n",
    "        \n",
    "        for obj in self.graph[curr][0]:\n",
    "            typ = self.z[obj]\n",
    "            self.update_ms(0, (curr_type, typ), -1)\n",
    "            m_bar[(curr_type, typ)] -= 1\n",
    "        for obj in self.graph[curr][1]:\n",
    "            typ = self.z[obj] \n",
    "            m[(curr_type, typ)] -= 1\n",
    "            self.update_ms(1, (curr_type, typ), -1)\n",
    "        for obj in self.graph[curr][2]:\n",
    "            typ = self.z[obj]\n",
    "            self.update_ms(0, (typ, curr_type), -1)\n",
    "            m_bar[(typ, curr_type)] -= 1\n",
    "        for obj in self.graph[curr][3]:\n",
    "            typ = self.z[obj] \n",
    "            self.update_ms(1, (typ, curr_type), -1)\n",
    "            m[(typ, curr_type)] -= 1\n",
    "\n",
    "        return {\"obj\" : curr, \"old_type\" : curr_type, \"new_type\" : None, \"m\" : m, \"m_bar\" : m_bar}\n",
    "         \n",
    "    def assign(self, obj, new_type):\n",
    "        \"\"\"Assign given object 'obj' to type 'new_type'.\n",
    "        \n",
    "        This function returns a dictionary called hist which is a log\n",
    "        of changes made when the assignment happens. It allows undoing\n",
    "        or redoing the particular assignment.\n",
    "        \"\"\"\n",
    "        curr = obj\n",
    "\n",
    "        m = Counter()\n",
    "        m_bar = Counter()\n",
    "    \n",
    "        self.z[curr] = new_type\n",
    "        self.obj_type_mem[new_type].add(curr)\n",
    "        self.obj_type_inc(new_type) \n",
    "        \n",
    "        if self.graph[curr][4] == 1:\n",
    "            self.update_ms(1, (new_type, new_type), 1)\n",
    "            m[(new_type, new_type)] += 1     \n",
    "        elif self.graph[curr][4] == 0:\n",
    "            self.update_ms(0, (new_type, new_type), 1)\n",
    "            m_bar[(new_type, new_type)] += 1\n",
    "            \n",
    "        for obj in self.graph[curr][0]:\n",
    "            typ = self.z[obj]\n",
    "            self.update_ms(0, (new_type, typ), 1)\n",
    "            m_bar[(new_type, typ)] += 1\n",
    "        for obj in self.graph[curr][1]:\n",
    "            typ = self.z[obj]\n",
    "            self.update_ms(1, (new_type, typ), 1)\n",
    "            m[(new_type, typ)] += 1\n",
    "        for obj in self.graph[curr][2]:\n",
    "            typ = self.z[obj]\n",
    "            self.update_ms(0, (typ, new_type), 1)\n",
    "            m_bar[(typ, new_type)] += 1\n",
    "        for obj in self.graph[curr][3]:\n",
    "            typ = self.z[obj]\n",
    "            self.update_ms(1, (typ, new_type), 1)\n",
    "            m[(typ, new_type)] += 1\n",
    "        \n",
    "        return {\"obj\" : curr, \"old_type\" : None, \"new_type\" : new_type, \"m\" : m, \"m_bar\" : m_bar}\n",
    "    \n",
    "    def revert(self, hist):\n",
    "        \"\"\"Undo the assignment logged by hist.\"\"\"\n",
    "        self.z[hist[\"obj\"]] = hist[\"old_type\"]\n",
    "\n",
    "        if hist[\"new_type\"] != None:\n",
    "            self.obj_type_mem[hist[\"new_type\"]].remove(hist[\"obj\"])\n",
    "            self.obj_type_dec(hist[\"new_type\"])\n",
    "        if hist[\"old_type\"] != None:\n",
    "            self.obj_type_mem[hist[\"old_type\"]].add(hist[\"obj\"])\n",
    "            self.obj_type_inc(hist[\"old_type\"])\n",
    "        \n",
    "        for pair in hist[\"m\"]:\n",
    "            self.update_ms(1, pair, -hist[\"m\"][pair])   \n",
    "        for pair in hist[\"m_bar\"]:\n",
    "            self.update_ms(0, pair, -hist[\"m_bar\"][pair])\n",
    "    \n",
    "    def reassign(self, hist):\n",
    "        \"\"\"Redo the assignment logged by hist.\"\"\"\n",
    "        self.z[hist[\"obj\"]] = hist[\"new_type\"]\n",
    "        if hist[\"new_type\"] != None:\n",
    "            self.obj_type_mem[hist[\"new_type\"]].add(hist[\"obj\"])\n",
    "            self.obj_type_inc(hist[\"new_type\"])\n",
    "        if hist[\"old_type\"] != None:\n",
    "            self.obj_type_mem[hist[\"old_type\"]].remove(hist[\"obj\"])\n",
    "            self.obj_type_dec(hist[\"old_type\"])\n",
    "        \n",
    "        for pair in hist[\"m\"]:\n",
    "            self.update_ms(1, pair, hist[\"m\"][pair])\n",
    "        for pair in hist[\"m_bar\"]:\n",
    "            self.update_ms(0, pair, hist[\"m_bar\"][pair])\n",
    "    \n",
    "    def Beta_factor(self, a, b):\n",
    "        \"\"\"Beta(m(a,b) + β, mbar(a,b) + β)/Beta(β,β)\"\"\"\n",
    "        beta = self.beta\n",
    "        alpha = self.alpha\n",
    "        return Beta(self.m[(a,b)] + alpha, self.m_bar[(a,b)] + beta) / Beta(alpha, beta)\n",
    "        \n",
    "    def CRProb(self, obj, obj_type):\n",
    "        \"\"\"p(z[obj] = obj_type | z[-obj]).\n",
    "        \n",
    "        z[-obj] : all z except z[obj].\n",
    "        Conditional Probability of object 'obj' being in cluster 'obj_type'\n",
    "        as given by by the CRP distribution. \n",
    "        \"\"\"\n",
    "        if obj_type not in self.obj_types:\n",
    "            return self.gamma / self.CRProb_cond_denom\n",
    "\n",
    "        prob = self.obj_type_count[obj_type]/ self.CRProb_cond_denom\n",
    "        return prob\n",
    "    \n",
    "    def Prob_Rz(self):\n",
    "        \"\"\"p(R|z)\"\"\"\n",
    "        prob = 1\n",
    "        for (a, b) in self.pairs:\n",
    "            prob *= self.Beta_factor(a, b)\n",
    "        return prob\n",
    "    \n",
    "    def Prob_zi(self, obj, obj_type):\n",
    "        \"\"\"p(z[obj] = obj_type | R, z[-obj]).\n",
    "        \n",
    "        p(z_{i}=a|z_{-i},R) ∝ p(R|z)p(z_{i}=a|z_{-i})\n",
    "        \"\"\"\n",
    "        prob = self.CRProb(obj, obj_type)\n",
    "        \n",
    "        # assign obj to obj_type, compute probability and undo assignment\n",
    "        hist = self.assign(obj, obj_type)\n",
    "        prob *= self.Prob_Rz()\n",
    "        self.revert(hist)\n",
    "        return prob, hist\n",
    "    \n",
    "    def Prob_z(self):\n",
    "        \"\"\"p(z)\"\"\"\n",
    "        prob = 1\n",
    "        for typ in self.obj_type_count:\n",
    "            num = self.obj_type_count[typ]\n",
    "            prob *= self.gamma\n",
    "            prob *= np.prod(np.arange(1, num))\n",
    "        prob /= self.CRProb_joint_denom\n",
    "        return prob\n",
    "        \n",
    "    def Prob_joint(self):\n",
    "        \"\"\"p(z,R) = p(R|z)p(z).\n",
    "        \n",
    "        When calculating π(z*)/π(z) = p(z*|R)/p(z|R) for Metropolis-Hasting update,\n",
    "        we use p(z*,R)/p(z|R) since p(R) in denoms cancel one another.\n",
    "        \"\"\"\n",
    "        prob = self.Prob_Rz()\n",
    "        prob *= self.Prob_z()\n",
    "        return prob\n",
    "        \n",
    "    def sample_zi(self, obj, alg = Sampler.GIBBS, restricted = False, type_opts = None, temp = None):\n",
    "        \"\"\"Sample cluster assignment for object 'obj'.\n",
    "        \n",
    "        If climb is True, the cluster for 'obj' is chosen greedily.\n",
    "        Clutser assignment can be restricted to those in 'type_opts'.\n",
    "        \"\"\"\n",
    "        types = []\n",
    "        probs = []\n",
    "        hists = {}\n",
    "        \n",
    "        # deassign obj to prepare for sampling\n",
    "        if alg == Sampler.ANNEAL:\n",
    "            old_prob = self.Prob_joint()\n",
    "            log = self.deassign(obj)\n",
    "        else:\n",
    "            self.deassign(obj)\n",
    "        \n",
    "        if not restricted:\n",
    "            # compute probabilities p(z_{obj}|z_{-obj},R) for existing clusters\n",
    "            for typ in self.obj_types:\n",
    "                prob, hist = self.Prob_zi(obj, typ)\n",
    "                types.append(typ)\n",
    "                probs.append(prob)\n",
    "                hists[typ] = hist\n",
    "\n",
    "            # compute probability p(z_{obj}|z_{-obj},R) for new clusters\n",
    "            new_table = self.cluster_ids[0]\n",
    "            prob, hist = self.Prob_zi(obj, new_table)\n",
    "            types.append(new_table)\n",
    "            probs.append(prob)\n",
    "            hists[new_table] = hist\n",
    "        else:\n",
    "            for typ in type_opts:\n",
    "                prob, hist = self.Prob_zi(obj, typ)\n",
    "                types.append(typ)\n",
    "                probs.append(prob)\n",
    "                hists[typ] = hist\n",
    "        \n",
    "        # normalize probabilities to make sum 1\n",
    "        probs /= sum(probs)\n",
    "        \n",
    "        # sample object type and reassign obj to sampled type\n",
    "        if alg == Sampler.CLIMB:\n",
    "            sample_id = np.argmax(probs)\n",
    "        else:\n",
    "            sample_id = npr.choice(np.arange(len(probs)), size = 1, p = probs)[0]\n",
    "        sample = types[sample_id]\n",
    "\n",
    "        self.reassign(hists[sample])\n",
    "\n",
    "        if alg == Sampler.ANNEAL:\n",
    "            new_prob = self.Prob_joint()\n",
    "            ap = np.exp((new_prob - old_prob)/temp)\n",
    "            if ap <= npr.random():\n",
    "                self.revert(hists[sample])\n",
    "                self.revert(log)\n",
    "\n",
    "        # compute joint and update max prob if necessary\n",
    "        joint = self.Prob_joint()\n",
    "        if joint > self.max_prob:\n",
    "            self.max_prob = joint\n",
    "            self.best_z = deepcopy(self.z)\n",
    "        \n",
    "        return probs[sample_id]                       \n",
    "    \n",
    "    def sim_sample_zi(self, obj, type_ori, type_opts):\n",
    "        \"\"\"Sample cluster assignment for object 'obj'.\"\"\"\n",
    "        types = []\n",
    "        probs = []\n",
    "        hists = {}\n",
    "        \n",
    "        # deassign obj to prepare for sampling\n",
    "        self.deassign(obj)\n",
    "        \n",
    "        # get probabilities for assigning obj\n",
    "        for typ in type_opts:\n",
    "            prob, hist = self.Prob_zi(obj, typ)\n",
    "            types.append(typ)\n",
    "            probs.append(prob)\n",
    "            hists[typ] = hist\n",
    "                \n",
    "        # normalize probabilities to make sum 1\n",
    "        probs /= sum(probs)\n",
    "        \n",
    "        # reassign obj to original type 'type_ori'\n",
    "        self.reassign(hists[type_ori])\n",
    "        \n",
    "        return probs[type_opts.index(type_ori)] \n",
    "\n",
    "    def climb_scan(self):\n",
    "        \"\"\"Greedy type assignment for objects.\n",
    "\n",
    "        Iteratively assign each object to best possible cluster given\n",
    "        current state.\n",
    "        \"\"\"\n",
    "        for obj in self.objs:\n",
    "            self.sample_zi(obj, alg = Sampler.CLIMB)\n",
    "\n",
    "    def gibbs_scan(self, restricted, objs, type_opts):\n",
    "        \"\"\"A single gibbs sampling scan.\n",
    "        \n",
    "        If restricted, implement a restricted sampling scan assigning\n",
    "        objs to one of type_opts.\n",
    "        \"\"\"\n",
    "        prob = 1\n",
    "        if restricted:\n",
    "            for obj in objs:\n",
    "                prob *= self.sample_zi(obj, restricted = restricted, type_opts = type_opts)\n",
    "        else:\n",
    "            for obj in self.objs:\n",
    "                prob *= self.sample_zi(obj, restricted = restricted, type_opts = type_opts)\n",
    "        return prob\n",
    "\n",
    "    def anneal_scan(self, temp):\n",
    "        for obj in self.objs:\n",
    "            self.sample_zi(obj, alg = Sampler.ANNEAL, temp = temp)\n",
    "        \n",
    "    def get_z(self):\n",
    "        return self.z\n",
    "    \n",
    "    def get_S(self, obj_types, exclude):\n",
    "        \"\"\"Get all objs belonging to either of obj_types.\n",
    "        \n",
    "        Exclude objects in exclude.\n",
    "        \"\"\"\n",
    "        objs = []\n",
    "        Sij = deepcopy(self.obj_type_mem[obj_types[0]])\n",
    "        if obj_types[0] != obj_types[1]:\n",
    "            Sij |= self.obj_type_mem[obj_types[1]]\n",
    "\n",
    "        for obj in self.z:\n",
    "            if obj not in exclude and self.z[obj] in obj_types:\n",
    "                objs.append(obj) \n",
    "        return objs\n",
    "\n",
    "    def rand_obj(self, z_i, num_objs):\n",
    "        \"\"\"Sample num_objs unique objects from type z_i.\"\"\"\n",
    "        return npr.choice(list(self.obj_type_mem[z_i]), size = num_objs, replace = False)\n",
    "    \n",
    "    def rand_z(self, restricted = False, objs = None, options = None):\n",
    "        \"\"\"Randomly assign objs to one of options.\n",
    "        \n",
    "        If not restricted, assign to each object a random type.\n",
    "        \"\"\"\n",
    "        if not restricted:\n",
    "            for obj in self.objs:\n",
    "                typ = npr.randint(self.num_objs)\n",
    "                self.deassign(obj)\n",
    "                self.assign(obj, typ)\n",
    "        else:\n",
    "            for obj in objs:\n",
    "                typ = npr.choice(options)\n",
    "                self.deassign(obj)\n",
    "                self.assign(obj, typ)\n",
    "    \n",
    "    def merge_sm(self, obj_ij, obj_types, t):\n",
    "        \"\"\"Implement merge process for Metropolis-Hastings Update.\"\"\"\n",
    "        # remember orginal assignments z\n",
    "        # required to compute q(z|z_merge)\n",
    "        ori_z = deepcopy(self.z)\n",
    "        \n",
    "        # perform t restricted gibbs sampling for objects in S\n",
    "        S = self.get_S(obj_types, obj_ij)\n",
    "        self.rand_z(True, S, obj_types)\n",
    "        self.sample_restr(t, S, obj_types)\n",
    "        \n",
    "        # compute q(z|z_merge)\n",
    "        prob = 1\n",
    "        for obj in S:\n",
    "            prob *= self.sim_sample_zi(obj, ori_z[obj], obj_types)\n",
    "        \n",
    "        # merge all objects in S∪{obj_i, obj_j} by assigning all to z_j\n",
    "        self.deassign(obj_ij[0])\n",
    "        self.assign(obj_ij[0], obj_types[1])\n",
    "        for obj in S:\n",
    "            if self.z[obj] != obj_types[1]:\n",
    "                self.deassign(obj)\n",
    "                self.assign(obj, obj_types[1])\n",
    "            \n",
    "        # return ratio of transitional probabilities \n",
    "        # q(z|z_merge)/q(z_merge/z) = q(z|z_merge)\n",
    "        return prob\n",
    "\n",
    "\n",
    "    def merge_opt(self, z_i, z_j):\n",
    "        \"\"\"Merge objects with types z_i and z_j into single cluster \n",
    "        with type z_i.\"\"\"\n",
    "        S = self.get_S((z_j, z_j), [])\n",
    "\n",
    "        for obj in S:\n",
    "            self.deassign(obj)\n",
    "            self.assign(obj, z_i)\n",
    "\n",
    "    def split_sm(self, obj_ij, z_j, t):\n",
    "        \"\"\"Implement merge process for Metropolis-Hastings Update.\"\"\"\n",
    "        # assign obj_i to a new cluster\n",
    "        z_i = self.cluster_ids[0]\n",
    "        self.deassign(obj_ij[0])\n",
    "        self.assign(obj_ij[0], z_i)\n",
    "        \n",
    "        # perform t restricted gibbs sampling for objects in S\n",
    "        S = self.get_S((z_i,z_j), obj_ij)\n",
    "        self.rand_z(True, S, (z_i,z_j))\n",
    "        self.sample_restr(t, S, (z_i,z_j))\n",
    "        \n",
    "        # return ratio of transitional probabilities \n",
    "        # q(z|z_split)/q(z_split/z) = 1/q(z|z_split)\n",
    "        return 1 / self.gibbs_scan(True, S, (z_i,z_j))\n",
    "\n",
    "    def split_opt(self, obj_ij, z_i):\n",
    "        \"\"\"Split the objects in type z_i into two clusters.\n",
    "\n",
    "        Use obj_i and obj_j as reference objects for the types\n",
    "        \"\"\"\n",
    "        z_j = self.cluster_ids[0]\n",
    "        self.deassign(obj_ij[1])\n",
    "        self.assign(obj_ij[1], z_j)\n",
    "\n",
    "        S = self.get_S((z_i,z_j), obj_ij)\n",
    "        self.rand_z(True, S, (z_i,z_j))\n",
    "        \n",
    "    def split_merge(self, t):\n",
    "        \"\"\"split-merge Metroplolis-Hastings Update.\n",
    "        \n",
    "        t = number of intermediate restricted Gibbs sampling scans.\n",
    "        \"\"\"\n",
    "        # compute π(z) = p(z|R) ∝ p(z,R)\n",
    "        pi_z = self.Prob_joint()\n",
    "        \n",
    "        # backup current state\n",
    "        self.backup()\n",
    "        \n",
    "        # randomly choose two objects\n",
    "        obj_i, obj_j = npr.choice(self.objs, size = 2, replace = False)\n",
    "        z_i, z_j = self.z[obj_i], self.z[obj_j]\n",
    "        \n",
    "        # execute split or merge \n",
    "        if z_i == z_j:\n",
    "            trans_prob = self.split_sm((obj_i, obj_j), z_j, t)\n",
    "        else:\n",
    "            trans_prob = self.merge_sm((obj_i, obj_j), (z_i,z_j), t)\n",
    "        \n",
    "        # compute π(z*) = p(z*|R) ∝ p(z*,R)\n",
    "        pi_z_new = self.Prob_joint()\n",
    "        \n",
    "        # a(z*,z) = min(1, transitional probability * π(z*) / π(z))\n",
    "        accep_prob = min(1, trans_prob * pi_z_new / pi_z)\n",
    "        \n",
    "        # accept or reject\n",
    "        accept = npr.binomial(1, accep_prob)\n",
    "        if accept:\n",
    "            self.empty_backup()\n",
    "        else:\n",
    "            self.restore()\n",
    "    \n",
    "    def opt_split(self, alg = Sampler.CLIMB, temp = None):\n",
    "        \"\"\"Attempt splitting each class.\n",
    "\n",
    "        In order to run fast enough for big datasets, we randomly choose a \n",
    "        a type num_types times weighted by the cluster size.\n",
    "        \"\"\"\n",
    "        types = []\n",
    "        sizes = []\n",
    "        num_types = len(self.obj_types)\n",
    "\n",
    "        for typ in self.obj_types:\n",
    "            types.append(typ)\n",
    "            sizes.append(self.obj_type_count[typ])\n",
    "\n",
    "        # normalize cluster sizes to sum to 1\n",
    "        sizes /= np.sum(sizes)\n",
    "\n",
    "        for _ in range(num_types):\n",
    "            old_prob = self.Prob_joint()\n",
    "            self.backup()\n",
    "\n",
    "            # choose an object type \n",
    "            z_i = npr.choice(types, size = 1, p = sizes)[0]\n",
    "\n",
    "            if self.obj_type_count[z_i] == 1:\n",
    "                continue\n",
    "\n",
    "            # pick two objects from the type and split using them\n",
    "            obj_i, obj_j = self.rand_obj(z_i, 2)\n",
    "            self.split_opt((obj_i, obj_j), z_i)\n",
    "\n",
    "            new_prob = self.Prob_joint()\n",
    "\n",
    "            if alg == Sampler.CLIMB:\n",
    "                if new_prob <= old_prob:\n",
    "                    self.restore()\n",
    "            else:\n",
    "                ap = np.exp((new_prob - old_prob)/temp)\n",
    "                if ap <= npr.random():\n",
    "                    self.restore()\n",
    "            self.empty_backup()\n",
    "\n",
    "    def opt_merge(self, alg = Sampler.CLIMB, temp = None):\n",
    "        \"\"\"Attempt merging classes with one other.\n",
    "\n",
    "        In order to run fast enough for big datasets, for each type we \n",
    "        randomly choose another type and attempt merge.\n",
    "        \"\"\"\n",
    "        if len(self.obj_types) == 1:\n",
    "            return\n",
    "\n",
    "        types = list(deepcopy(self.obj_types))\n",
    "        for z_i in types:\n",
    "            z_j = z_i\n",
    "\n",
    "            while z_j == z_i:\n",
    "                z_j = npr.choice(types)\n",
    "\n",
    "            old_prob = self.Prob_joint()\n",
    "            self.backup()\n",
    "\n",
    "            self.merge_opt(z_i, z_j)\n",
    "            new_prob = self.Prob_joint()\n",
    "\n",
    "            if alg == Sampler.CLIMB:\n",
    "                if new_prob <= old_prob:\n",
    "                    self.restore()\n",
    "            else:\n",
    "                ap = np.exp((new_prob - old_prob)/temp)\n",
    "                if ap <= npr.random():\n",
    "                    self.restore()\n",
    "            self.empty_backup()\n",
    "\n",
    "    def sample_gibbs(self, num_iters):\n",
    "        \"\"\"Run num_iters gibbs sampling scans.\"\"\"\n",
    "        for i in range(num_iters):\n",
    "            a.gibbs_scan(False, None, None)\n",
    "    \n",
    "    def sample_restr(self, num_iters, objs, type_opts):\n",
    "        \"\"\"Run num_ites restricted gibbs sampling scans.\"\"\"\n",
    "        for i in range(num_iters):\n",
    "            a.gibbs_scan(True, objs, type_opts)\n",
    "\n",
    "    def sample_climb(self, num_iter):\n",
    "        \"\"\"Sample using hillclimbing with random restarts\"\"\"\n",
    "        old_prob = -np.inf\n",
    "        new_prob = -np.inf\n",
    "        num_repeats = 0\n",
    "\n",
    "        for _ in tqdm(range(num_iter)):\n",
    "            # move each class to the best type\n",
    "            self.climb_scan()\n",
    "\n",
    "            # attempt cluster splits\n",
    "            self.opt_split()\n",
    "\n",
    "            # attempt cluster merges\n",
    "            self.opt_merge()\n",
    "\n",
    "            new_prob = np.log(self.Prob_joint())\n",
    "\n",
    "            # if no substantial changes for last 8 iters, restart with\n",
    "            # random cluster assignments\n",
    "            if abs(new_prob - old_prob) < 0.00001 * abs(new_prob):\n",
    "                num_repeats += 1\n",
    "                if num_repeats == 8:\n",
    "                    num_repeats = 0\n",
    "                    self.rand_z()\n",
    "            else:\n",
    "                num_repeats = 0\n",
    "\n",
    "            old_prob = new_prob\n",
    "\n",
    "    def sample_anneal(self, num_iter):\n",
    "        T = 1.0\n",
    "        T_min = 0.0001\n",
    "        rate = 0.9\n",
    "\n",
    "        while T > T_min:\n",
    "            for _ in range(num_iter):\n",
    "                self.anneal_scan(T)\n",
    "                self.opt_split(Sampler.ANNEAL, T)\n",
    "                self.opt_merge(Sampler.ANNEAL, T)\n",
    "            T *= rate\n",
    "\n",
    "    def sample_full(self, num_iter, inter, split_merge, gibbs):\n",
    "        \"\"\"Sample using both Metropolis-Hastings update and Gibbs \n",
    "        sampling Scans.\n",
    "        \n",
    "        num_iters = number of complete sampling iterations\n",
    "        iter = number of intermediate restricted gibbs samplings\n",
    "               per split-merge update\n",
    "        split_merge = number of split-merge update per iteration\n",
    "        gibbs = number of gibbs sampling scans per iteration\n",
    "        \"\"\"\n",
    "        for _ in tqdm(range(num_iter)):\n",
    "            for _ in range(split_merge):\n",
    "                self.split_merge(inter)\n",
    "            self.sample_gibbs(gibbs)\n",
    "            \n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sue': 0, 'fred': 0, 'mary': 0, 'ann': 0, 'tom': 0, 'jim': 0}\n",
      "1.3875013875013875e-05\n"
     ]
    }
   ],
   "source": [
    "a = model(1,1,1,rel)\n",
    "print(a.get_z())\n",
    "print(a.Prob_joint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.sample_anneal(10)\n",
    "print(a.get_z())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.sample_full(1, 5, 1, 1)\n",
    "print(a.get_z())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.sample_climb(1)\n",
    "print(a.get_z())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rel = [('tom', 'fred', 1), ('tom', 'jim', 1), ('jim', 'fred', 1), ('mary', 'fred', 0),('mary', 'jim', 0),\n",
    "       ('sue', 'fred', 0) , ('sue', 'tom', 0) ,('ann', 'jim', 0), ('ann', 'tom', 0), ('mary', 'sue', 1) ,\n",
    "       ('mary', 'ann', 1) ,('ann', 'sue', 1)]\n",
    "magnets = [(\"a1\",\"a1\",1),(\"a1\",\"a2\",1),(\"a2\",\"a1\",1),(\"a2\",\"a2\",1),\n",
    "(\"b1\",\"b1\",0),(\"b1\",\"b2\",0),(\"b2\",\"b1\",0),(\"b2\",\"b2\",0),\n",
    "(\"c1\",\"c1\",0),(\"c1\",\"c2\",0),(\"c2\",\"c1\",0),(\"c2\",\"c2\",0),\n",
    "(\"a1\",\"b1\",0),(\"a1\",\"b2\",0),(\"a2\",\"b1\",0),(\"a2\",\"b2\",0),\n",
    "(\"b1\",\"a1\",0),(\"b1\",\"a2\",0),(\"b2\",\"a1\",0),(\"b2\",\"a2\",0),\n",
    "(\"a1\",\"c1\",1),(\"a1\",\"c2\",1),(\"a2\",\"c1\",1),(\"a2\",\"c2\",1),\n",
    "(\"c1\",\"a1\",1),(\"c1\",\"a2\",1),(\"c2\",\"a1\",1),(\"c2\",\"a2\",1),\n",
    "(\"b1\",\"c1\",0),(\"b1\",\"c2\",0),(\"b2\",\"c1\",0),(\"b2\",\"c2\",0),\n",
    "(\"c1\",\"b1\",0),(\"c1\",\"b2\",0),(\"c2\",\"b1\",0),(\"c2\",\"b2\",0)]\n",
    "magnets2 = [(\"a1\",\"a2\",1),(\"a2\",\"a1\",0),\n",
    "(\"b1\",\"b1\",0),(\"b1\",\"b2\",0),(\"b2\",\"b1\",0),(\"b2\",\"b2\",0),\n",
    "(\"c1\",\"c1\",0),(\"c1\",\"c2\",1),(\"c2\",\"c2\",0),\n",
    "(\"a1\",\"b1\",0),(\"a2\",\"b1\",0),(\"a2\",\"b2\",0),\n",
    "(\"b1\",\"a1\",0),(\"b2\",\"a1\",1),(\"b2\",\"a2\",0),\n",
    "(\"a1\",\"c1\",1),(\"a1\",\"c2\",1),(\"a2\",\"c1\",1),(\"a2\",\"c2\",0),\n",
    "(\"c1\",\"a1\",1),(\"c1\",\"a2\",0),(\"c2\",\"a1\",1),(\"c2\",\"a2\",1),\n",
    "(\"b1\",\"c2\",0),(\"b2\",\"c1\",0),(\"b2\",\"c2\",0),\n",
    "(\"c1\",\"b1\",1),(\"c2\",\"b1\",1),(\"c2\",\"b2\",0)]\n",
    "magnets3 = [(\"a1\",\"a1\",1),(\"a1\",\"a2\",1),(\"a2\",\"a1\",0),(\"a2\",\"a2\",1),\n",
    "(\"b1\",\"b1\",0),(\"b1\",\"b2\",0),(\"b2\",\"b1\",0),(\"b2\",\"b2\",0),\n",
    "(\"c1\",\"c1\",0),(\"c1\",\"c2\",1),(\"c2\",\"c1\",0),(\"c2\",\"c2\",0),\n",
    "(\"a1\",\"b1\",0),(\"a1\",\"b2\",0),(\"a2\",\"b1\",0),(\"a2\",\"b2\",0),\n",
    "(\"b1\",\"a1\",0),(\"b1\",\"a2\",0),(\"b2\",\"a1\",1),(\"b2\",\"a2\",0),\n",
    "(\"a1\",\"c1\",1),(\"a1\",\"c2\",1),(\"a2\",\"c1\",1),(\"a2\",\"c2\",1),\n",
    "(\"c1\",\"a1\",1),(\"c1\",\"a2\",0),(\"c2\",\"a1\",1),(\"c2\",\"a2\",1),\n",
    "(\"b1\",\"c1\",0),(\"b1\",\"c2\",0),(\"b2\",\"c1\",0),(\"b2\",\"c2\",0),\n",
    "(\"c1\",\"b1\",1),(\"c1\",\"b2\",0),(\"c2\",\"b1\",1),(\"c2\",\"b2\",0)]\n",
    "magnets4 = [(\"a1\",\"a1\",1),(\"a1\",\"a2\",1),(\"a2\",\"a1\",0),(\"a2\",\"a2\",1),\n",
    "(\"b1\",\"b1\",0),(\"b1\",\"b2\",0),(\"b2\",\"b1\",0),(\"b2\",\"b2\",0),\n",
    "(\"c1\",\"c1\",0),(\"c1\",\"c2\",0),(\"c2\",\"c1\",0),(\"c2\",\"c2\",1),\n",
    "(\"a1\",\"b1\",0),(\"a1\",\"b2\",0),(\"a2\",\"b1\",0),(\"a2\",\"b2\",0),\n",
    "(\"b1\",\"a1\",0),(\"b1\",\"a2\",0),(\"b2\",\"a1\",1),(\"b2\",\"a2\",0),\n",
    "(\"a1\",\"c1\",1),(\"a1\",\"c2\",1),(\"a2\",\"c1\",1),(\"a2\",\"c2\",0),\n",
    "(\"c1\",\"a1\",1),(\"c1\",\"a2\",0),(\"c2\",\"a1\",1),(\"c2\",\"a2\",1),\n",
    "(\"b1\",\"c1\",0),(\"b1\",\"c2\",0),(\"b2\",\"c1\",0),(\"b2\",\"c2\",0),\n",
    "(\"c1\",\"b1\",0),(\"c1\",\"b2\",0),(\"c2\",\"b1\",1),(\"c2\",\"b2\",0)]\n",
    "magnets5 = [(\"a1\",\"a1\",1),(\"a1\",\"a2\",1),(\"a2\",\"a1\",1),(\"a2\",\"a2\",1),\n",
    "(\"b1\",\"b1\",0),(\"b1\",\"b2\",0),(\"b2\",\"b1\",0),(\"b2\",\"b2\",0),\n",
    "(\"c1\",\"c1\",0),(\"c1\",\"c2\",0),(\"c2\",\"c1\",0),(\"c2\",\"c2\",0),\n",
    "(\"a1\",\"b1\",0),(\"a1\",\"b2\",0),(\"a2\",\"b1\",0),(\"a2\",\"b2\",0),\n",
    "(\"b1\",\"a1\",0),(\"b1\",\"a2\",1),(\"b2\",\"a1\",0),(\"b2\",\"a2\",0),\n",
    "(\"a1\",\"c1\",1),(\"a1\",\"c2\",1),(\"a2\",\"c1\",1),(\"a2\",\"c2\",1),\n",
    "(\"c1\",\"a1\",1),(\"c1\",\"a2\",1),(\"c2\",\"a1\",1),(\"c2\",\"a2\",0),\n",
    "(\"b1\",\"c1\",0),(\"b1\",\"c2\",0),(\"b2\",\"c1\",0),(\"b2\",\"c2\",0),\n",
    "(\"c1\",\"b1\",0),(\"c1\",\"b2\",0),(\"c2\",\"b1\",0),(\"c2\",\"b2\",0)]\n",
    "magnets6 = [(\"a1\",\"a2\",1),(\"a2\",\"a2\",1),\n",
    "(\"b1\",\"b1\",0),(\"b1\",\"b2\",0),(\"b2\",\"b1\",0),(\"b2\",\"b2\",0),\n",
    "(\"c1\",\"c1\",0),(\"c1\",\"c2\",0),(\"c2\",\"c2\",0),\n",
    "(\"a1\",\"b1\",0),(\"a2\",\"b1\",0),(\"a2\",\"b2\",0),\n",
    "(\"b1\",\"a1\",0),(\"b2\",\"a1\",1),(\"b2\",\"a2\",0),\n",
    "(\"a1\",\"c1\",1),(\"a1\",\"c2\",1),(\"a2\",\"c1\",1),(\"a2\",\"c2\",1),\n",
    "(\"c1\",\"a1\",1),(\"c2\",\"a1\",1),(\"c2\",\"a2\",1),\n",
    "(\"b1\",\"c2\",0),(\"b2\",\"c1\",0),\n",
    "(\"c1\",\"b1\",0),(\"c2\",\"b1\",0),(\"c2\",\"b2\",0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "blocks = [(\"a1\",\"a1\",1),(\"a1\",\"a2\",1), (\"a3\",\"a3\",1), (\"a4\",\"a4\",1), (\"a1\",\"a3\",1), (\"a2\",\"a3\",1),\n",
    "          (\"a2\",\"a1\",1),(\"a2\",\"a4\",1),(\"a4\",\"a1\",1),(\"a4\",\"a3\",1),(\"a4\",\"a5\",1),(\"a5\",\"a4\",1),(\"a5\",\"a2\",1),\n",
    "          (\"b1\",\"b2\",0),(\"b2\",\"b3\",0),(\"b3\",\"b1\",0), (\"b1\",\"b1\",0),(\"b2\",\"b2\",0),(\"b3\",\"b2\",0),(\"b3\",\"b1\",0),\n",
    "          (\"a1\",\"b1\",0),(\"a1\",\"b3\",0),(\"b2\",\"a1\",0),(\"b2\",\"a3\",0),(\"a4\",\"b2\",0),(\"b2\",\"a5\",0),(\"a5\",\"b2\",0),\n",
    "          (\"a3\",\"b3\",0),(\"b3\",\"a3\",0),(\"c1\",\"c3\",0),(\"c3\",\"c1\",0),(\"c1\",\"c2\",0),(\"c2\",\"c1\",0),(\"c3\",\"c2\",0),\n",
    "          (\"c3\",\"c4\",0),(\"c4\",\"c1\",0),(\"c4\",\"c2\",0),(\"c1\",\"c4\",0),(\"b1\",\"c1\",0),(\"c2\",\"b1\",0),(\"b3\",\"c4\",0),\n",
    "          (\"b1\",\"c4\",0),(\"c3\",\"b1\",0),(\"b2\",\"c3\",0),(\"c3\",\"b1\",0),(\"a1\",\"c1\",1),(\"a1\",\"c3\",1),(\"c2\",\"a1\",1),\n",
    "          (\"a3\",\"c1\",1),(\"c4\",\"a3\",1),(\"a5\",\"a3\",1),(\"a4\",\"c1\",1),(\"c1\",\"a4\",1),(\"a4\",\"c4\",1),(\"a5\",\"c2\",1),\n",
    "          (\"c2\",\"a3\",1), (\"a2\",\"c1\",1),(\"c2\",\"c2\",0),(\"c3\",\"c3\",0),(\"c4\",\"c4\",0),(\"c1\",\"c1\",0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "blocks2 = [(\"a1\",\"a1\",1),(\"a1\",\"a2\",1), (\"a3\",\"a3\",1), (\"a4\",\"a4\",0), (\"a1\",\"a3\",1), (\"a2\",\"a3\",0), (\"a2\",\"a1\",0),(\"a2\",\"a4\",1),(\"a4\",\"a1\",1),(\"a4\",\"a3\",0),(\"a4\",\"a5\",1),(\"a5\",\"a4\",0),(\"a5\",\"a2\",1),(\"b1\",\"b2\",0),(\"b2\",\"b3\",0),(\"b3\",\"b1\",0), (\"b1\",\"b1\",0),(\"b2\",\"b2\",0),(\"b3\",\"b2\",1),(\"b3\",\"b1\",0),(\"a1\",\"b1\",0),(\"a1\",\"b3\",0),(\"b2\",\"a1\",1),(\"b2\",\"a3\",0),(\"a4\",\"b2\",0),(\"b2\",\"a5\",0),(\"a5\",\"b2\",1),(\"a3\",\"b3\",0),(\"b3\",\"a3\",0),(\"c1\",\"c3\",0),(\"c3\",\"c1\",1),(\"c1\",\"c2\",0),(\"c2\",\"c1\",0),(\"c3\",\"c2\",0),(\"c3\",\"c4\",0),(\"c4\",\"c1\",1),(\"c4\",\"c2\",0),(\"c1\",\"c4\",0),(\"b1\",\"c1\",0),(\"c2\",\"b1\",0),(\"b3\",\"c4\",0),(\"b1\",\"c4\",0),(\"c3\",\"b1\",1),(\"b2\",\"c3\",0),(\"c3\",\"b1\",0),(\"a1\",\"c1\",1),(\"a1\",\"c3\",1),(\"c2\",\"a1\",1),(\"a3\",\"c1\",0),(\"c4\",\"a3\",1),(\"a5\",\"a3\",1),(\"a4\",\"c1\",1),(\"c1\",\"a4\",0),(\"a4\",\"c4\",0),(\"a5\",\"c2\",1),(\"c2\",\"a3\",1), (\"a2\",\"c1\",0),(\"c2\",\"c2\",0),(\"c3\",\"c3\",0),(\"c4\",\"c4\",0),(\"c1\",\"c1\",1)]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [my-rdkit-env]",
   "language": "python",
   "name": "Python [my-rdkit-env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
